{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semantic_search.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKrT6Mahxk3u"
      },
      "source": [
        "# Beyond `CTRL+F`\n",
        "__Implementing semantic search within a document using word embeddings__\n",
        "\n",
        "<table align=\"center\">\n",
        "    <tr><th>\n",
        "        <a href=\"https://colab.research.google.com/github/fabio-a-oliveira/semantic-search/blob/master/semantic_search.ipynb\">\n",
        "            <img src=\"https://github.com/fabio-a-oliveira/semantic-search/blob/main/data/colab_logo_32px.png?raw=true\">\n",
        "            <br>Run in Google Colab\n",
        "        </a>\n",
        "    </th></tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zOAG0hv0bo9"
      },
      "source": [
        "---\n",
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2nSJzR50eqs"
      },
      "source": [
        "In this notebook, we'll apply _Natural Language Processing (NLP)_ techniques to implement a semantic search within a document. This means that, instead of searching for a literal word or sequence of words (as you would when you use `CTRL+F` in Notepad, Microsoft Word etc), we'll be searching for terms, sentences or paragraphs of ___similar meaning___. If you ever had to search a massive document for a piece of information that you cannot recall exactly, you will agree that this application can be extremely useful and time-saving.\n",
        "\n",
        "In summary, this is what we are going to do:\n",
        "\n",
        "1. Use ___word embeddings___ to convert every word in the book and the requested sentence to a dense vector representation (in this case, we'll use GloVe embeddings);\n",
        "2. Apply a ___part-of-speech___ (POS) mask to both the book and the requested sentence: every word that does not belong to a list of relevant _parts-of-speech_ will have the embedding converted to a null vector;\n",
        "3. Apply a ___bag-of-words___ approach to sentence embedding: average the embeddings of every word in the sentence and get a single vector to represent its semantic content;\n",
        "4. Apply the _bag-of-words_ sentence embedding and POS filter to the entire book by using a ___sliding window___ with the length of the requested sentence plus a selected margin and averaging the embeddings of words within the window;\n",
        "5. Calculate the ___cosine distance___ between the requested sentence embedding and the sliding window embeddings\n",
        "6. Select the position in the book with the shortest distance to the requested sentence.\n",
        "\n",
        "To illustrate the concept, we'll download the .txt file of the book _Pride and Prejudice_ from the Project Gutenberg website and we'll show two applications of the technique:\n",
        "\n",
        "1. We will take a sentence from the book and search the text for several increasingly altered versions of it;\n",
        "2. We will take several excerpts from the Brazilian Portuguese version of the book, translate them back to English (which results in sentences with equivalent meaning but significantly different wording), and find the correspoding match in the original version.\n",
        "\n",
        "Finally, a web form is provided so that you can experiment with the technique in any book available in the ___Project Gutenberg___ catalogue of public domain works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_qBGgbwEGCX"
      },
      "source": [
        "---\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsDA6R2s8n75"
      },
      "source": [
        "In this section, we will do all the preparation and definitions necessary for the analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJohjJmwEMu3"
      },
      "source": [
        "### Install and import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srZb7Ujh8vzf"
      },
      "source": [
        "We begin by importanting the required libraries. Most of the NLP resources we need are available in the `nltk` (Natural Language Toolkit) package. We also need to install and import the `googletrans` library, which will be useful for translating text from Portuguese to English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PP5RRFWoCfXD",
        "outputId": "f870e667-3dc2-406b-ea96-1f2ffa1fa6c3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from os import mkdir, getcwd, chdir, listdir\n",
        "from os.path import join\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "import pkg_resources\n",
        "from matplotlib import pyplot as plt\n",
        "from zipfile import ZipFile\n",
        "from textwrap import wrap"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1lzaRjZGb6b",
        "outputId": "f49350c1-1bc6-4150-d5ec-0eece90fa8c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "package_list = [pkg for pkg in pkg_resources.working_set]\n",
        "\n",
        "if 'googletrans' not in [pkg.key for pkg in package_list]:\n",
        "    ! pip install googletrans==3.1.0a0\n",
        "import googletrans"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/3d/4e3a1609bf52f2f7b00436cc751eb977e27040665dde2bd57e7152989672/googletrans-3.1.0a0.tar.gz\n",
            "Collecting httpx==0.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 285kB/s \n",
            "\u001b[?25hRequirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Collecting httpcore==0.9.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Collecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n",
            "Collecting hstspreload\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/50/606213e12fb49c5eb667df0936223dcaf461f94e215ea60244b2b1e9b039/hstspreload-2020.12.22-py3-none-any.whl (994kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.12.5)\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/78/be/7b8b99fd74ff5684225f50dd0e865393d2265656ef3b4ba9eaaaffe622b8/rfc3986-1.4.0-py2.py3-none-any.whl\n",
            "Collecting h2==3.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.4MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.8MB/s \n",
            "\u001b[?25hCollecting hpack<4,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-cp37-none-any.whl size=16368 sha256=ff54973df52b3e2734d33f2b763e2336ece2e0d9155a56986ff3441fe4182320\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/7a/a0/aff3babbb775549ce6813cb8fa7ff3c0848c4dc62c20f8fdac\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hpack, hyperframe, h2, sniffio, h11, httpcore, hstspreload, rfc3986, httpx, googletrans\n",
            "Successfully installed googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.12.22 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.4.0 sniffio-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWfP_Yyg9bnX"
      },
      "source": [
        "I also like to declare a `HOME` variable, so that we have a handle to the starting directory available throughout the analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNrVK2N8G5TA"
      },
      "source": [
        "HOME = getcwd()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "494OcGIIFhLS"
      },
      "source": [
        "### Download and prepare word embeddings and Project Gutenberg catalogue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGXH3GW8_9Sx"
      },
      "source": [
        "The heart and soul of capturing word meaning will be provided by GloVe word embeddings. In order to use them, we download the embeddings from the Stanford NLP website and convert the raw data to a Python dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAV2EaMqFq5A"
      },
      "source": [
        "if 'GloVe' not in listdir():\n",
        "    mkdir('GloVe')\n",
        "\n",
        "if 'glove.6B.zip' not in listdir(join(HOME, 'GloVe')):\n",
        "    URL_GloVe = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "    r = requests.get(URL_GloVe).content\n",
        "    with open(join('GloVe', 'glove.6B.zip'), 'wb') as file:\n",
        "        file.write(r)\n",
        "\n",
        "z = ZipFile(join(HOME, 'GloVe','glove.6B.zip'))\n",
        "z.extractall(join(HOME, 'GloVe'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP3wimK0PWYs"
      },
      "source": [
        "embedding_dict = {}\n",
        "\n",
        "with open(join(HOME, 'GloVe', 'glove.6B.300d.txt')) as file:\n",
        "    for line in file:\n",
        "        word, vector = line.split(' ', maxsplit=1)\n",
        "        vector = np.array(vector.split(' ')).astype('float')\n",
        "        embedding_dict.update({word:vector})"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYJ8196XQ8xW"
      },
      "source": [
        "We also need to download references to the Project Gutenberg catalogue, containing URLs for each entry in its huge corpus of public domain books."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaR8dZ7rRSXy"
      },
      "source": [
        "if 'catalogue' not in listdir():\n",
        "    mkdir('catalogue')\n",
        "\n",
        "if 'project_gutenberg_corpus.csv' not in listdir(join(HOME,'catalogue')):\n",
        "    URL = \"https://github.com/fabio-a-oliveira/semantic-search/blob/main/data/project_gutenberg_corpus___2021_05_01.csv?raw=true\"\n",
        "    content = requests.get(URL).content\n",
        "    with open(join(HOME,'catalogue','project_gutenberg_corpus.csv'), 'wb') as file:\n",
        "        file.write(content)\n",
        "\n",
        "catalogue = pd.read_csv(join(HOME, 'catalogue', 'project_gutenberg_corpus.csv'),\n",
        "                        sep = \"|\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe849Ejz9FgH"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGWFG9WgAlI8"
      },
      "source": [
        "We now define a series of functions that will be useful in performing the analysis repeatedly.\n",
        "\n",
        "We begin with a function that takes text content as a string and returns a matrix with the embeddings for each word according to a given dictionary.\n",
        "\n",
        "In this step, most applications use a list of ___stop words___ - frequently occurrying words that can be removed from the bag-of-words representation without much loss of meaning. My personal preference is for using a list of allowed parts-of-speech, which gives me more control over what categories of words will be kept or masked out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUF5FsnF9EvT"
      },
      "source": [
        "def sequence_embedding(content, embedding_dict, \n",
        "                       allowed_pos = ['NN','NNS','NNP','NNPS','JJ','RB','VB',\n",
        "                                      'VBG','VBN','VBP','VBZ','VBD']):\n",
        "\n",
        "    # basic cleanup\n",
        "    content = content.replace('\\n', ' ').replace('_', \"\").lower()\n",
        "\n",
        "    # tokenize content\n",
        "    tokens = nltk.word_tokenize(content)\n",
        "\n",
        "    # get mask indicating tokens that are in or out of allowed parts-of-speech\n",
        "    pos_mask = np.array([int(tag[1] in allowed_pos) \n",
        "                         for tag in nltk.pos_tag(tokens)]).reshape((-1,1))\n",
        "\n",
        "    # get embedding and apply mask\n",
        "    embedding_dim = len(list(embedding_dict.values())[0])\n",
        "    embedding = np.array([embedding_dict[token] \n",
        "                          if token in embedding_dict.keys() \n",
        "                          else np.zeros(embedding_dim) \n",
        "                          for token in tokens])\n",
        "    embedding *= pos_mask\n",
        "\n",
        "    return embedding"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdFNjlspBueF"
      },
      "source": [
        "We also define a function to calculate the _cosine distance_ between embeddings. This function could be easily accessed from the `scipy` package, but since I intend to use the same code to deploy a simple web app I prefer to define it using `numpy` to preclude the need to import an additional library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsIOK3Hl9EYL"
      },
      "source": [
        "def cosine_distance(vec1, vec2):\n",
        "\n",
        "    dot_prod = np.dot(vec1, vec2)\n",
        "    norm1 = np.sqrt(np.sum(vec1 ** 2))\n",
        "    norm2 = np.sqrt(np.sum(vec2 ** 2))\n",
        "\n",
        "    if norm1 == 0 or norm2 == 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 1 - dot_prod / (norm1 * norm2)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hssEaVl0ChDV"
      },
      "source": [
        "Next, we define a function that calculates the cosine distance between the embedding corresponding to the request excerpt and the embeddings of a sliding window of words throughout the entire book.\n",
        "\n",
        "The length of the sliding window corresponds to the length of the given excerpt plus a selected margin. The margin is implemented to account for the fact that sometimes our recollection of a particular excerpt is limited, and the actual corresponding text is probably longer than the provided input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0MufHxLEHMn"
      },
      "source": [
        "def sliding_distance(book, excerpt, margin = 0):\n",
        "\n",
        "    excerpt_length = excerpt.shape[0]\n",
        "    mvg_avg_widgth = excerpt_length + margin\n",
        "\n",
        "    # bag-of-words excerpt embedding\n",
        "    excerpt_embedding = excerpt.mean(axis=0)\n",
        "\n",
        "    # moving average of the book embedding, \n",
        "    # considering the length of the excerpt + a margin\n",
        "    mvg_avg_embedding = np.array([book[line-mvg_avg_widgth:line].mean(axis=0) \n",
        "                            for line in range(mvg_avg_widgth, book.shape[0])])\n",
        "\n",
        "    # sliding distance: distance between sliding window and excerpt embeddings\n",
        "    distance = np.array([cosine_distance(mvg_avg_embedding[line,:], \n",
        "                                         excerpt_embedding) \n",
        "                         for line in range(mvg_avg_embedding.shape[0])])\n",
        "\n",
        "    return distance"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp5FKOe5DTnh"
      },
      "source": [
        "The next function takes a chosen position and sentence length and returns the corresponding excerpt from the book. This will be used after we have applied the `sliding_distance()` function to calculate the distances between the requested excerpt and each possible sentence in the book and applied the `ndarray.argmin()` method to find the position with the smallest distance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI5woBLuIrhW"
      },
      "source": [
        "def find_match(reference, match_position, match_length):\n",
        "\n",
        "    # basic cleanup\n",
        "    reference = reference.replace('\\n', ' ').replace('_', \"\").lower()\n",
        "\n",
        "    match = nltk.word_tokenize(reference)\n",
        "    match = match[match_position : match_position + match_length]\n",
        "    match = ' '.join(match).replace(' ,',',').replace(' .','.')\n",
        "    return match"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3kvvWd4D5oy"
      },
      "source": [
        "Finally, the `locate_excerpt()` function puts it all together: it receives a desired excerpt and a book embedding as input and returns the matching sentence from the book. It also accepts a `margin` argument, which makes it easier to search for sentences that are longer than the provided excerpt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5Jgrdf7YfxP"
      },
      "source": [
        "def locate_excerpt(excerpt, book, margin = 0):\n",
        "    # embed excerpt and get word count\n",
        "    book_embedding = sequence_embedding(book, embedding_dict)\n",
        "    excerpt_embedding = sequence_embedding(excerpt, embedding_dict)\n",
        "    excerpt_word_count = len(nltk.word_tokenize(excerpt))\n",
        "\n",
        "    # calculate distances\n",
        "    distances = sliding_distance(book_embedding, \n",
        "                                 excerpt_embedding, \n",
        "                                 margin = margin)\n",
        "\n",
        "    # find match and print it\n",
        "    match = find_match(book, distances.argmin(), \n",
        "                       excerpt_word_count + margin)\n",
        "    \n",
        "    return match"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX-2mUtLETna"
      },
      "source": [
        "---\n",
        "## Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTfrOhjQE3cu"
      },
      "source": [
        "Now that we have everything we need to begin searching for text with similar semantic content inside a document, let's apply the technique to Jane Austen's _Pride and Prejudice_.\n",
        "\n",
        "In the first application, we'll take a sentence from the book and search the document for several modified versions of it.\n",
        "\n",
        "In the second, we'll take several sentences from the Brazilian Portuguese translation of the book, translate them back to English (which results in excerpts with similar meaning but very different wordings) and search the book for them.\n",
        "\n",
        "Finally, you will be able to use a web form to apply the technique to any book publicly available via the Project Gutenberg website."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHXkk_NUVRMy"
      },
      "source": [
        "### Search for excerpts from a given input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uh27GaYFqno"
      },
      "source": [
        "We first need to download the book from the Project Gutenberg website and convert every word to its dense vector representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHR7rw1PAJhP"
      },
      "source": [
        "URL = \"https://www.gutenberg.org/cache/epub/42671/pg42671.txt\"\n",
        "book = str(requests.get(URL).content, encoding='utf-8')\n",
        "book_embedding = sequence_embedding(book, embedding_dict)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVUqAwJ3F0eY"
      },
      "source": [
        "The excerpt we'll be searching for is the first sentence in the book:\n",
        "\n",
        "> ___\"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\"___\n",
        "\n",
        "We'll begin by searching for the original sentence, just to get a feel for the technique and make sure it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QtYh6Q6CYvF1",
        "outputId": "f2a27f64-cc2d-4150-d393-7f0680f1f560"
      },
      "source": [
        "excerpt = (\"It is a truth universally acknowledged, that a single man \" + \n",
        "          \"in possession of a good fortune, must be in want of a wife.\")\n",
        "locate_excerpt(excerpt, book)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'it is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s87BPYPGfxE"
      },
      "source": [
        "Well, it is no surprise that the search works. After all, we were searching for the exact same sentence. Let's make it a little more difficult and change some of the words for synonims:\n",
        "\n",
        "> ___It is a fact universally known, that an unmarried man in possession of a vast fortune, must be in need of a wife___"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7fnAM1eNG_Rx",
        "outputId": "7de8dccb-4a45-47f3-d5cd-953a9e5f9e4d"
      },
      "source": [
        "excerpt = (\"It is a fact universally known, that an unmarried man in \" + \n",
        "          \"possession of a vast fortune, must be in need of a wife\")\n",
        "locate_excerpt(excerpt, book)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. however little known'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e-1darKI3B7"
      },
      "source": [
        "Great! We do have a match with the corresponding original sentence. However, we can see that there is a slight misalignment in the result: the first 4 words are missing, and 3 additional words (or 4 tokens, considering the dot) are added to the end. This is a frequent artifact of the response. Since we are working with sliding windows, there is a large overlap between sentences, and results will be often ofset by a few words to the right or left.\n",
        "\n",
        "Now, let's go beyond synonims and alter the sentence more substantially, including some simplifications:\n",
        "\n",
        "> ___It is a fact universally known, that a rich and unmarried man surely needs of a wife.___\n",
        "\n",
        "Because we are providing an excerpt that is somewhat shorter than the original, adding a margin to increase the length of the sliding window will help in locating the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "j_8zjj72ZKoX",
        "outputId": "adfdac20-b362-404a-8d64-18f49a3e9699"
      },
      "source": [
        "excerpt = (\"It is a fact universally known, that a man who is rich \" + \n",
        "          \"and single surely wants a wife\")\n",
        "locate_excerpt(excerpt, book, margin = 10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'it is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. however little known'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YUs1a1VKi4A"
      },
      "source": [
        "We have a match again!\n",
        "\n",
        "Now, let's try an extreme example and provide just the gist of the sentence as an excerpt for the algorithm:\n",
        "\n",
        "> ___Everyone knows that a rich single man wants a wife.___"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IZhdmnHyZKXL",
        "outputId": "03138a1e-a4fa-4dae-f7fe-269c15ce3edb"
      },
      "source": [
        "excerpt = \"Everyone knows that a rich single man wants a wife\"\n",
        "locate_excerpt(excerpt, book, margin = 10)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'as he chooses. nobody wants him to come. though i shall always say that he used my daughter'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGJ9-NfLK_on"
      },
      "source": [
        "That did not go very well. The algorithm found a match that does not correspond to the original sentence we wanted in the book.\n",
        "\n",
        "However, we can cycle through some of the top results and see if the actual sentence pops up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIvPFuxZMz9n",
        "outputId": "5ff960a0-2995-46db-f796-877d92797af0"
      },
      "source": [
        "# define excerpt\n",
        "excerpt = \"Everyone knows that a rich single man wants a wife\"\n",
        "margin = 10\n",
        "\n",
        "# embed excerpt and get word count\n",
        "excerpt_embedding = sequence_embedding(excerpt, embedding_dict)\n",
        "excerpt_word_count = len(nltk.word_tokenize(excerpt))\n",
        "\n",
        "# calculate distances\n",
        "distances = sliding_distance(book_embedding, excerpt_embedding, margin = margin)\n",
        "\n",
        "# find top 20 results and print them\n",
        "for i in range(20):\n",
        "    print(\"{}) \".format(i+1), end ='')\n",
        "    print(find_match(book, distances.argsort()[i], \n",
        "                     excerpt_word_count + 2*margin))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1) as he chooses. nobody wants him to come. though i shall always say that he used my daughter extremely ill ; and if i was her, i\n",
            "2) he is such a man ! '' `` yes, yes, they must marry. there is nothing else to be done. but there are two things that\n",
            "3) is such a man ! '' `` yes, yes, they must marry. there is nothing else to be done. but there are two things that i\n",
            "4) he chooses. nobody wants him to come. though i shall always say that he used my daughter extremely ill ; and if i was her, i would\n",
            "5) , that a single man in possession of a good fortune, must be in want of a wife. however little known the feelings or views of such a\n",
            "6) acknowledged, that a single man in possession of a good fortune, must be in want of a wife. however little known the feelings or views of such\n",
            "7) a man ! '' `` yes, yes, they must marry. there is nothing else to be done. but there are two things that i want very\n",
            "8) man ! '' `` yes, yes, they must marry. there is nothing else to be done. but there are two things that i want very much\n",
            "9) chooses. nobody wants him to come. though i shall always say that he used my daughter extremely ill ; and if i was her, i would not\n",
            "10) , '' said her daughter, `` she would go. '' `` she is a very fine-looking woman ! and her calling here was prodigiously civil ! for she\n",
            "11) it, '' said her daughter, `` she would go. '' `` she is a very fine-looking woman ! and her calling here was prodigiously civil ! for\n",
            "12) '' said her daughter, `` she would go. '' `` she is a very fine-looking woman ! and her calling here was prodigiously civil ! for she only\n",
            "13) said her daughter, `` she would go. '' `` she is a very fine-looking woman ! and her calling here was prodigiously civil ! for she only came\n",
            "14) perpetually talked of. my mother means well ; but she does not know, no one can know how much i suffer from what she says. happy shall\n",
            "15) that a single man in possession of a good fortune, must be in want of a wife. however little known the feelings or views of such a man\n",
            "16) such a man ! '' `` yes, yes, they must marry. there is nothing else to be done. but there are two things that i want\n",
            "17) a single man in possession of a good fortune, must be in want of a wife. however little known the feelings or views of such a man may\n",
            "18) room, `` and what do you think of my husband ? is not he a charming man ? i am sure my sisters must all envy me. i\n",
            "19) , well ! it is just as he chooses. nobody wants him to come. though i shall always say that he used my daughter extremely ill ; and\n",
            "20) every thing else she is as good natured a girl as ever lived. i will go directly to mr. bennet, and we shall very soon settle it with\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-2WOBiAOZXT"
      },
      "source": [
        "Looking carefully at the top 20 results, we see that the original excerpt we were looking for corresponds to entries 5, 6, 15, and 17. There is a lot of overlap between results, so there are actually just 7 different parts in this selection, of which our desired outcome is the third.\n",
        "\n",
        "It would be fairly simple to write a function that identifies these overlaps and merges them into single results. In that case, our desired excerpt would be the third result on the list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuCzcWloVWn_"
      },
      "source": [
        "### Search for original text from translations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR0UDaibY8kX"
      },
      "source": [
        "We'll now move to a slightly different application, where instead of searching for sentences manually modified from the original book, we will look for excerpts corresponding to the Brazilian Portuguese translation.\n",
        "\n",
        "For each of these excerpts, we will use the `googletrans` library to automatically translate them back into English. We will then use our search routine to try and find the corresponding excerpt in the original book. \n",
        "\n",
        "You will notice that the translation is rather different from the original text. Although it certainly holds the same meaning, the choice and order of words is remarkably different.\n",
        "\n",
        "We begin with a few single sentences, starting with the first sentence in the book and then trying a few more:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJJ_ChTSZ84b",
        "outputId": "10ffba2b-3661-45ea-c814-85b9cf944c17"
      },
      "source": [
        "excerpt_translated = (\"É uma verdade universalmente conhecida que um homem \" + \n",
        "                     \"solteiro, possuidor de uma boa fortuna, deve estar \" + \n",
        "                     \"necessitado de esposa.\")\n",
        "excerpt_english = googletrans.Translator().translate(excerpt_translated).text\n",
        "result = locate_excerpt(excerpt_english, book)\n",
        "\n",
        "print(\"Excerpt (pt):\")\n",
        "[print(st) for st in wrap(excerpt_translated)];\n",
        "print(\"\\nTranslation (en):\")\n",
        "[print(st) for st in wrap(excerpt_english)];\n",
        "print(\"\\nOriginal (en):\")\n",
        "[print(st) for st in wrap(result)];"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Excerpt (pt):\n",
            "É uma verdade universalmente conhecida que um homem solteiro,\n",
            "possuidor de uma boa fortuna, deve estar necessitado de esposa.\n",
            "\n",
            "Translation (en):\n",
            "It is a universally known truth that a single man, possessing a good\n",
            "fortune, must be in need of a wife.\n",
            "\n",
            "Original (en):\n",
            "is a truth universally acknowledged, that a single man in possession\n",
            "of a good fortune, must be in want of a wife\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6QWksXEZ8yv",
        "outputId": "8c5940bc-82e8-4c8a-c0e0-e352be892d3b"
      },
      "source": [
        "excerpt_translated = (\"Mas quando Elizabeth contou que ele ficara em \" + \n",
        "                     \"silêncio, a hipótese não pareceu muito plausível, mesmo \"+ \n",
        "                     \"para Charlotte que a desejava.\")\n",
        "excerpt_english = googletrans.Translator().translate(excerpt_translated).text\n",
        "result = locate_excerpt(excerpt_english, book)\n",
        "\n",
        "print(\"Excerpt (pt):\")\n",
        "[print(st) for st in wrap(excerpt_translated)];\n",
        "print(\"\\nTranslation (en):\")\n",
        "[print(st) for st in wrap(excerpt_english)];\n",
        "print(\"\\nOriginal (en):\")\n",
        "[print(st) for st in wrap(result)];"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Excerpt (pt):\n",
            "Mas quando Elizabeth contou que ele ficara em silêncio, a hipótese não\n",
            "pareceu muito plausível, mesmo para Charlotte que a desejava.\n",
            "\n",
            "Translation (en):\n",
            "But when Elizabeth told him he was silent, the hypothesis did not seem\n",
            "very plausible, even to Charlotte who wanted it.\n",
            "\n",
            "Original (en):\n",
            "when elizabeth told of his silence, it did not seem very likely, even\n",
            "to charlotte 's wishes, to be the case\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYkim046Z8ve",
        "outputId": "e6b25841-c3fa-43bd-fa55-13b03175e67a"
      },
      "source": [
        "excerpt_translated = (\"Mrs. Gardiner ficou surpreendida e preocupada. Mas \" + \n",
        "                     \"como se aproximavam agora do lugar onde residira na \" + \n",
        "                     \"sua mocidade, ela se entregou toda ao encanto das \" + \n",
        "                     \"suas recordações\")\n",
        "excerpt_english = googletrans.Translator().translate(excerpt_translated).text\n",
        "result = locate_excerpt(excerpt_english, book)\n",
        "\n",
        "print(\"Excerpt (pt):\")\n",
        "[print(st) for st in wrap(excerpt_translated)];\n",
        "print(\"\\nTranslation (en):\")\n",
        "[print(st) for st in wrap(excerpt_english)];\n",
        "print(\"\\nOriginal (en):\")\n",
        "[print(st) for st in wrap(result)];"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Excerpt (pt):\n",
            "Mrs. Gardiner ficou surpreendida e preocupada. Mas como se aproximavam\n",
            "agora do lugar onde residira na sua mocidade, ela se entregou toda ao\n",
            "encanto das suas recordações\n",
            "\n",
            "Translation (en):\n",
            "Mrs. Gardiner was surprised and concerned. But as they now approached\n",
            "the place where she had resided in her youth, she gave herself over to\n",
            "the charm of her memories\n",
            "\n",
            "Original (en):\n",
            "on. mrs. gardiner was surprised and concerned ; but as they were now\n",
            "approaching the scene of her former pleasures, every idea gave way to\n",
            "the charm of recollection ;\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXl-cBDqZ8rM",
        "outputId": "a2e45b2a-de9d-4574-fea7-5fc349df6d91"
      },
      "source": [
        "excerpt_translated = (\"— Acha que eles estão em Londres? — Sim, em que \" + \n",
        "                     \"outro lugar poderiam se esconder?\")\n",
        "excerpt_english = googletrans.Translator().translate(excerpt_translated).text\n",
        "result = locate_excerpt(excerpt_english, book)\n",
        "\n",
        "print(\"Excerpt (pt):\")\n",
        "[print(st) for st in wrap(excerpt_translated)];\n",
        "print(\"\\nTranslation (en):\")\n",
        "[print(st) for st in wrap(excerpt_english)];\n",
        "print(\"\\nOriginal (en):\")\n",
        "[print(st) for st in wrap(result)];"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Excerpt (pt):\n",
            "— Acha que eles estão em Londres? — Sim, em que outro lugar poderiam\n",
            "se esconder?\n",
            "\n",
            "Translation (en):\n",
            "- Do you think they're in London? - Yes, where else could they hide?\n",
            "\n",
            "Original (en):\n",
            ". '' `` do you suppose them to be in london ? '' `` yes ; where else\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq1wRlTOaPI2"
      },
      "source": [
        "In all of these examples, the method was able to identify the correct excerpt from the original text (give or take a few words to the right or left), even though the translations were very different from the original.\n",
        "\n",
        "Finally, we try a longer excerpt with several sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-7LguoBZ8m-",
        "outputId": "e5710384-a876-4700-b46f-db609ff06e91"
      },
      "source": [
        "excerpt_translated = (\"Se pudéssemos saber quais eram as dívidas de \" + \n",
        "                     \"Wickham... E com quanto ele dotou nossa irmã... \" + \n",
        "                     \"Saberia exatamente o que Mr. Gardiner fez, pois \" + \n",
        "                     \"Wickham não tem um tostão de seu. A bondade dos nossos \"+\n",
        "                     \"tios é uma coisa que nunca poderá ser paga. Eles a \" + \n",
        "                     \"levaram para casa e lhe deram toda a sua proteção e \" + \n",
        "                     \"apoio moral. Isto é um sacrifício que anos de gratidão \" +\n",
        "                     \"não podem compensar. Nesse momento, ela está em casa \" + \n",
        "                     \"deles. Se uma tão grande bondade não lhe der a \" + \n",
        "                     \"consciência da falta que praticou, é que ela não \" + \n",
        "                     \"merece nunca ser feliz. Imagina a sua cara quando \" + \n",
        "                     \"chegar diante da minha tia\")\n",
        "excerpt_english = googletrans.Translator().translate(excerpt_translated).text\n",
        "result = locate_excerpt(excerpt_english, book)\n",
        "\n",
        "print(\"Excerpt (pt):\")\n",
        "[print(st) for st in wrap(excerpt_translated)];\n",
        "print(\"\\nTranslation (en):\")\n",
        "[print(st) for st in wrap(excerpt_english)];\n",
        "print(\"\\nOriginal (en):\")\n",
        "[print(st) for st in wrap(result)];"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Excerpt (pt):\n",
            "Se pudéssemos saber quais eram as dívidas de Wickham... E com quanto\n",
            "ele dotou nossa irmã... Saberia exatamente o que Mr. Gardiner fez,\n",
            "pois Wickham não tem um tostão de seu. A bondade dos nossos tios é uma\n",
            "coisa que nunca poderá ser paga. Eles a levaram para casa e lhe deram\n",
            "toda a sua proteção e apoio moral. Isto é um sacrifício que anos de\n",
            "gratidão não podem compensar. Nesse momento, ela está em casa deles.\n",
            "Se uma tão grande bondade não lhe der a consciência da falta que\n",
            "praticou, é que ela não merece nunca ser feliz. Imagina a sua cara\n",
            "quando chegar diante da minha tia\n",
            "\n",
            "Translation (en):\n",
            "If we could know what Wickham's debts were ... And how much he endowed\n",
            "our sister with ... He would know exactly what Mr. Gardiner did,\n",
            "because Wickham doesn't have a penny of his own. The kindness of our\n",
            "uncles is something that can never be paid for. They took her home and\n",
            "gave her all her protection and moral support. This is a sacrifice\n",
            "that years of gratitude cannot make up for. Right now, she is at their\n",
            "home. If such great kindness does not make her aware of the fault she\n",
            "has committed, it is that she never deserves to be happy. Imagine her\n",
            "face when you arrive in front of my aunt\n",
            "\n",
            "Original (en):\n",
            "to learn what wickham 's debts have been, '' said elizabeth, `` and\n",
            "how much is settled on his side on our sister, we shall exactly know\n",
            "what mr. gardiner has done for them, because wickham has not sixpence\n",
            "of his own. the kindness of my uncle and aunt can never be requited.\n",
            "their taking her home, and affording her their personal protection and\n",
            "countenance, is such a sacrifice to her advantage, as years of\n",
            "gratitude can not enough acknowledge. by this time she is actually\n",
            "with them ! if such goodness does not make her miserable now, she will\n",
            "never deserve to be happy ! what a meeting for her, when she first\n",
            "sees\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_60h-aaif0"
      },
      "source": [
        "Again, we see that the match was correctly found in the original text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOqf0HAPLfPS"
      },
      "source": [
        "### Your turn!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX-3a3i7Liz_"
      },
      "source": [
        "Now you can try searching for whatever you'd like from the Project Gutenberg catalog of public domain books.\n",
        "\n",
        "You can provide text for the search in any language, as it will be translated to English before searching the document.\n",
        "\n",
        "If you can't find the result you expected, ty experimenting with the `margin` parameter, as it will broaden the length of the sliding window.\n",
        "\n",
        "To run a search using the form, just fill it up and hit `CTRL+ENTER` or click on the _play_ icon on the left side of the form.\n",
        "\n",
        "Before running a search, be sure to run this entire notebook by going to the _Runtime_ menu and clicking _Run all_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "cIBh-wLILfcf",
        "outputId": "7cfaf021-f0f1-4c42-a485-e08bcb152fb7"
      },
      "source": [
        "#@title Choose your book, author and excerpt\n",
        "\n",
        "Author = \"Jane Austen\" #@param {type:\"string\"}\n",
        "Book = \"Pride and Prejudice\" #@param {type:\"string\"}\n",
        "Excerpt = \"it is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\" #@param {type:\"string\"}\n",
        "Margin = 0 #@param {type:\"slider\", min:0, max:50, step:1}\n",
        "\n",
        "excerpt_original = Excerpt\n",
        "excerpt = googletrans.Translator().translate(excerpt_original)\n",
        "src_lan = excerpt.src\n",
        "excerpt = excerpt.text\n",
        "\n",
        "no_author = False\n",
        "no_book = False\n",
        "\n",
        "df = catalogue\n",
        "df = df.loc[df.Language == 'en']\n",
        "for name in Author.replace(',',' ').split():\n",
        "    df = df.loc[df.Author.map(lambda x: name.lower() in x.lower())]\n",
        "if df.shape[0] == 0:\n",
        "    no_author = True\n",
        "\n",
        "for word in Book.split():\n",
        "    df = df.loc[df.Title.map(lambda x: word.lower() in x.lower())]\n",
        "if df.shape[0] == 0:\n",
        "    no_book = True\n",
        "\n",
        "if no_author:\n",
        "    print(\"Author not found\")\n",
        "elif no_book:\n",
        "    print(\"Book not found. Here are some available from the same author:\\n\")\n",
        "    df = catalogue\n",
        "    df = df.loc[df.Language == 'en']\n",
        "    for name in Author.replace(',',' ').split():\n",
        "        df = df.loc[df.Author.map(lambda x: name.lower() in x.lower())]\n",
        "    print(df.Title.values)\n",
        "else:\n",
        "    URL = df.URL.iloc[0]\n",
        "    selected_book = str(requests.get(URL).content, encoding='utf-8')\n",
        "    #selected_book_embedding = sequence_embedding(selected_book, embedding_dict)\n",
        "    result = locate_excerpt(excerpt, selected_book, margin = Margin)\n",
        "\n",
        "    print(\"Book URL: {}\\n\".format(URL))\n",
        "\n",
        "    if src_lan == 'en':\n",
        "        print(\"Provided excerpt:\")\n",
        "        [print(st) for st in wrap(excerpt_original)];\n",
        "        print(\"\\nOriginal:\")\n",
        "        [print(st) for st in wrap(result)];\n",
        "    else:\n",
        "        print(\"Provided excerpt ({}):\".format(src_lan))\n",
        "        [print(st) for st in wrap(excerpt_original)];\n",
        "        print(\"\\nTranslation (en):\")\n",
        "        [print(st) for st in wrap(excerpt)];\n",
        "        print(\"\\nOriginal (en):\")\n",
        "        [print(st) for st in wrap(result)];"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Book URL: https://www.gutenberg.org/files/1342/1342-0.txt\n",
            "\n",
            "Provided excerpt:\n",
            "it is a truth universally acknowledged, that a single man in\n",
            "possession of a good fortune, must be in want of a wife.\n",
            "\n",
            "Original:\n",
            "1 it is a truth universally acknowledged, that a single man in\n",
            "possession of a good fortune, must be in want of a wife\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tse2S69qarlL"
      },
      "source": [
        "---\n",
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t35g-aWardI"
      },
      "source": [
        "The method proposed in this notebook performed remarkably well at searching a document for an excerpt corresponding in meaning to a provided sentence. The heart and soul of the translation of words into a meaningful vector representation was provided by the GloVe word embeddings, and most of the NLP tools were used from the `nltk` package.\n",
        "\n",
        "We applied the technique to inputs corresponding to severely modified/simplified versions of a book passage, and also to excerpts from a book translation, both with similarly positive results.\n",
        "\n",
        "Admittedly, there was some cherry-picking in the presented results, especially in the selection of the `margin` parameter. In an actual application, it might be necessary for the user to try a couple of different values for this parameter or to scroll through a short list of candidate results. However, none of these are more complex than the current `CTRL+F` routine we all have to perform more often than we would like.\n",
        "\n",
        "With some minor modifications - like merging overlapping results - the technique could easily be applied to search for content in large documents, which is currently an ineffective and time-consuming task."
      ]
    }
  ]
}